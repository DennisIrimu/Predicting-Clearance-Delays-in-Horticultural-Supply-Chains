{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consignment_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>origin_country</th>\n",
       "      <th>destination_country</th>\n",
       "      <th>origin_port</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>shipment_mode</th>\n",
       "      <th>commodity</th>\n",
       "      <th>hs_code</th>\n",
       "      <th>gross_weight_kg</th>\n",
       "      <th>...</th>\n",
       "      <th>customs_release_hours</th>\n",
       "      <th>terminal_dwell_hours</th>\n",
       "      <th>sla_hours</th>\n",
       "      <th>total_processing_hours</th>\n",
       "      <th>delayed_flag</th>\n",
       "      <th>delay_hours</th>\n",
       "      <th>bl_or_awb_no</th>\n",
       "      <th>container_no</th>\n",
       "      <th>documents</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TLIP-SYN-000001</td>\n",
       "      <td>2025-02-21 09:38:50</td>\n",
       "      <td>RW</td>\n",
       "      <td>ES</td>\n",
       "      <td>Kigali</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Mangoes</td>\n",
       "      <td>804</td>\n",
       "      <td>7636.85</td>\n",
       "      <td>...</td>\n",
       "      <td>42.65</td>\n",
       "      <td>37.66</td>\n",
       "      <td>48</td>\n",
       "      <td>80.32</td>\n",
       "      <td>1</td>\n",
       "      <td>4.32</td>\n",
       "      <td>802-78096246</td>\n",
       "      <td></td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000001', 'doc_ty...</td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000001', 'event'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TLIP-SYN-000002</td>\n",
       "      <td>2025-01-13 05:46:46</td>\n",
       "      <td>RW</td>\n",
       "      <td>IT</td>\n",
       "      <td>Kigali</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Fresh Beans</td>\n",
       "      <td>708</td>\n",
       "      <td>21069.24</td>\n",
       "      <td>...</td>\n",
       "      <td>50.00</td>\n",
       "      <td>25.19</td>\n",
       "      <td>48</td>\n",
       "      <td>75.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>148-27192561</td>\n",
       "      <td></td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000002', 'doc_ty...</td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000002', 'event'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TLIP-SYN-000003</td>\n",
       "      <td>2025-09-19 04:40:34</td>\n",
       "      <td>TZ</td>\n",
       "      <td>NL</td>\n",
       "      <td>Dar es Salaam</td>\n",
       "      <td>Rotterdam</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Cut Flowers</td>\n",
       "      <td>603</td>\n",
       "      <td>15769.06</td>\n",
       "      <td>...</td>\n",
       "      <td>31.82</td>\n",
       "      <td>29.51</td>\n",
       "      <td>48</td>\n",
       "      <td>61.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>252-21633769</td>\n",
       "      <td></td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000003', 'doc_ty...</td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000003', 'event'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TLIP-SYN-000004</td>\n",
       "      <td>2025-12-04 07:26:32</td>\n",
       "      <td>RW</td>\n",
       "      <td>DE</td>\n",
       "      <td>Kigali</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Cut Flowers</td>\n",
       "      <td>603</td>\n",
       "      <td>21497.58</td>\n",
       "      <td>...</td>\n",
       "      <td>46.19</td>\n",
       "      <td>38.15</td>\n",
       "      <td>48</td>\n",
       "      <td>84.34</td>\n",
       "      <td>1</td>\n",
       "      <td>8.34</td>\n",
       "      <td>901-77635129</td>\n",
       "      <td></td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000004', 'doc_ty...</td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000004', 'event'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TLIP-SYN-000005</td>\n",
       "      <td>2025-05-08 16:09:41</td>\n",
       "      <td>ET</td>\n",
       "      <td>FR</td>\n",
       "      <td>Addis Ababa-ADD</td>\n",
       "      <td>Paris-CDG</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Pineapples</td>\n",
       "      <td>804</td>\n",
       "      <td>21298.46</td>\n",
       "      <td>...</td>\n",
       "      <td>58.94</td>\n",
       "      <td>85.32</td>\n",
       "      <td>96</td>\n",
       "      <td>144.26</td>\n",
       "      <td>1</td>\n",
       "      <td>20.26</td>\n",
       "      <td>BL823732870</td>\n",
       "      <td>XCDZ0728567</td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000005', 'doc_ty...</td>\n",
       "      <td>[{'consignment_id': 'TLIP-SYN-000005', 'event'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    consignment_id          created_at origin_country destination_country  \\\n",
       "0  TLIP-SYN-000001 2025-02-21 09:38:50             RW                  ES   \n",
       "1  TLIP-SYN-000002 2025-01-13 05:46:46             RW                  IT   \n",
       "2  TLIP-SYN-000003 2025-09-19 04:40:34             TZ                  NL   \n",
       "3  TLIP-SYN-000004 2025-12-04 07:26:32             RW                  DE   \n",
       "4  TLIP-SYN-000005 2025-05-08 16:09:41             ET                  FR   \n",
       "\n",
       "       origin_port destination_port shipment_mode    commodity  hs_code  \\\n",
       "0           Kigali           Madrid           AIR      Mangoes      804   \n",
       "1           Kigali            Genoa           AIR  Fresh Beans      708   \n",
       "2    Dar es Salaam        Rotterdam           AIR  Cut Flowers      603   \n",
       "3           Kigali        Frankfurt           AIR  Cut Flowers      603   \n",
       "4  Addis Ababa-ADD        Paris-CDG           SEA   Pineapples      804   \n",
       "\n",
       "   gross_weight_kg  ...  customs_release_hours terminal_dwell_hours  \\\n",
       "0          7636.85  ...                  42.65                37.66   \n",
       "1         21069.24  ...                  50.00                25.19   \n",
       "2         15769.06  ...                  31.82                29.51   \n",
       "3         21497.58  ...                  46.19                38.15   \n",
       "4         21298.46  ...                  58.94                85.32   \n",
       "\n",
       "   sla_hours  total_processing_hours  delayed_flag  delay_hours  bl_or_awb_no  \\\n",
       "0         48                   80.32             1         4.32  802-78096246   \n",
       "1         48                   75.19             0         0.00  148-27192561   \n",
       "2         48                   61.34             0         0.00  252-21633769   \n",
       "3         48                   84.34             1         8.34  901-77635129   \n",
       "4         96                  144.26             1        20.26   BL823732870   \n",
       "\n",
       "   container_no                                          documents  \\\n",
       "0                [{'consignment_id': 'TLIP-SYN-000001', 'doc_ty...   \n",
       "1                [{'consignment_id': 'TLIP-SYN-000002', 'doc_ty...   \n",
       "2                [{'consignment_id': 'TLIP-SYN-000003', 'doc_ty...   \n",
       "3                [{'consignment_id': 'TLIP-SYN-000004', 'doc_ty...   \n",
       "4   XCDZ0728567  [{'consignment_id': 'TLIP-SYN-000005', 'doc_ty...   \n",
       "\n",
       "                                              events  \n",
       "0  [{'consignment_id': 'TLIP-SYN-000001', 'event'...  \n",
       "1  [{'consignment_id': 'TLIP-SYN-000002', 'event'...  \n",
       "2  [{'consignment_id': 'TLIP-SYN-000003', 'event'...  \n",
       "3  [{'consignment_id': 'TLIP-SYN-000004', 'event'...  \n",
       "4  [{'consignment_id': 'TLIP-SYN-000005', 'event'...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"tlip_like_consignments_5000.jsonl\", lines=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 27 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   consignment_id          5000 non-null   object        \n",
      " 1   created_at              5000 non-null   datetime64[ns]\n",
      " 2   origin_country          5000 non-null   object        \n",
      " 3   destination_country     5000 non-null   object        \n",
      " 4   origin_port             5000 non-null   object        \n",
      " 5   destination_port        5000 non-null   object        \n",
      " 6   shipment_mode           5000 non-null   object        \n",
      " 7   commodity               5000 non-null   object        \n",
      " 8   hs_code                 5000 non-null   int64         \n",
      " 9   gross_weight_kg         5000 non-null   float64       \n",
      " 10  declared_value_usd      5000 non-null   float64       \n",
      " 11  exporter_profile        5000 non-null   object        \n",
      " 12  doc_completeness_score  5000 non-null   float64       \n",
      " 13  missing_docs_proxy      5000 non-null   int64         \n",
      " 14  doc_amendments          5000 non-null   int64         \n",
      " 15  congestion_index        5000 non-null   float64       \n",
      " 16  is_weekend_created      5000 non-null   int64         \n",
      " 17  customs_release_hours   5000 non-null   float64       \n",
      " 18  terminal_dwell_hours    5000 non-null   float64       \n",
      " 19  sla_hours               5000 non-null   int64         \n",
      " 20  total_processing_hours  5000 non-null   float64       \n",
      " 21  delayed_flag            5000 non-null   int64         \n",
      " 22  delay_hours             5000 non-null   float64       \n",
      " 23  bl_or_awb_no            5000 non-null   object        \n",
      " 24  container_no            5000 non-null   object        \n",
      " 25  documents               5000 non-null   object        \n",
      " 26  events                  5000 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(8), int64(6), object(12)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hs_code</th>\n",
       "      <th>gross_weight_kg</th>\n",
       "      <th>declared_value_usd</th>\n",
       "      <th>doc_completeness_score</th>\n",
       "      <th>missing_docs_proxy</th>\n",
       "      <th>doc_amendments</th>\n",
       "      <th>congestion_index</th>\n",
       "      <th>is_weekend_created</th>\n",
       "      <th>customs_release_hours</th>\n",
       "      <th>terminal_dwell_hours</th>\n",
       "      <th>sla_hours</th>\n",
       "      <th>total_processing_hours</th>\n",
       "      <th>delayed_flag</th>\n",
       "      <th>delay_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>818.200800</td>\n",
       "      <td>17355.272310</td>\n",
       "      <td>47651.335412</td>\n",
       "      <td>0.912280</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.431820</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>47.146678</td>\n",
       "      <td>51.454264</td>\n",
       "      <td>70.526400</td>\n",
       "      <td>98.600904</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>8.537048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>170.777984</td>\n",
       "      <td>7696.399211</td>\n",
       "      <td>27312.612578</td>\n",
       "      <td>0.058187</td>\n",
       "      <td>0.613705</td>\n",
       "      <td>0.764824</td>\n",
       "      <td>0.215877</td>\n",
       "      <td>0.448157</td>\n",
       "      <td>14.290495</td>\n",
       "      <td>22.939310</td>\n",
       "      <td>23.677733</td>\n",
       "      <td>34.580239</td>\n",
       "      <td>0.499787</td>\n",
       "      <td>12.833571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>603.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>18.860000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>708.000000</td>\n",
       "      <td>12100.437500</td>\n",
       "      <td>27729.320000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.717500</td>\n",
       "      <td>32.787500</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>70.760000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>804.000000</td>\n",
       "      <td>16929.755000</td>\n",
       "      <td>43391.385000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.925000</td>\n",
       "      <td>49.210000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>96.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>902.000000</td>\n",
       "      <td>22064.062500</td>\n",
       "      <td>62551.820000</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57.370000</td>\n",
       "      <td>70.062500</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>124.980000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1211.000000</td>\n",
       "      <td>51578.540000</td>\n",
       "      <td>224526.420000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.430000</td>\n",
       "      <td>127.540000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>201.060000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>77.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hs_code  gross_weight_kg  declared_value_usd  \\\n",
       "count  5000.000000      5000.000000         5000.000000   \n",
       "mean    818.200800     17355.272310        47651.335412   \n",
       "std     170.777984      7696.399211        27312.612578   \n",
       "min     603.000000      2000.000000         1600.000000   \n",
       "25%     708.000000     12100.437500        27729.320000   \n",
       "50%     804.000000     16929.755000        43391.385000   \n",
       "75%     902.000000     22064.062500        62551.820000   \n",
       "max    1211.000000     51578.540000       224526.420000   \n",
       "\n",
       "       doc_completeness_score  missing_docs_proxy  doc_amendments  \\\n",
       "count             5000.000000         5000.000000     5000.000000   \n",
       "mean                 0.912280            0.779000        0.569000   \n",
       "std                  0.058187            0.613705        0.764824   \n",
       "min                  0.713000            0.000000        0.000000   \n",
       "25%                  0.870000            0.000000        0.000000   \n",
       "50%                  0.914000            1.000000        0.000000   \n",
       "75%                  0.959000            1.000000        1.000000   \n",
       "max                  1.000000            3.000000        5.000000   \n",
       "\n",
       "       congestion_index  is_weekend_created  customs_release_hours  \\\n",
       "count       5000.000000         5000.000000            5000.000000   \n",
       "mean           0.431820            0.278200              47.146678   \n",
       "std            0.215877            0.448157              14.290495   \n",
       "min            0.000000            0.000000               6.000000   \n",
       "25%            0.261000            0.000000              36.717500   \n",
       "50%            0.414500            0.000000              46.925000   \n",
       "75%            0.587000            1.000000              57.370000   \n",
       "max            1.000000            1.000000              94.430000   \n",
       "\n",
       "       terminal_dwell_hours    sla_hours  total_processing_hours  \\\n",
       "count           5000.000000  5000.000000             5000.000000   \n",
       "mean              51.454264    70.526400               98.600904   \n",
       "std               22.939310    23.677733               34.580239   \n",
       "min                6.000000    48.000000               18.860000   \n",
       "25%               32.787500    48.000000               70.760000   \n",
       "50%               49.210000    48.000000               96.800000   \n",
       "75%               70.062500    96.000000              124.980000   \n",
       "max              127.540000    96.000000              201.060000   \n",
       "\n",
       "       delayed_flag  delay_hours  \n",
       "count   5000.000000  5000.000000  \n",
       "mean       0.483800     8.537048  \n",
       "std        0.499787    12.833571  \n",
       "min        0.000000     0.000000  \n",
       "25%        0.000000     0.000000  \n",
       "50%        0.000000     0.000000  \n",
       "75%        1.000000    14.292500  \n",
       "max        1.000000    77.060000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows (on hashable columns): 0\n",
      "\n",
      "Total duplicate rows (including first occurrence, on hashable columns): 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consignment_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>origin_country</th>\n",
       "      <th>destination_country</th>\n",
       "      <th>origin_port</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>shipment_mode</th>\n",
       "      <th>commodity</th>\n",
       "      <th>hs_code</th>\n",
       "      <th>gross_weight_kg</th>\n",
       "      <th>...</th>\n",
       "      <th>customs_release_hours</th>\n",
       "      <th>terminal_dwell_hours</th>\n",
       "      <th>sla_hours</th>\n",
       "      <th>total_processing_hours</th>\n",
       "      <th>delayed_flag</th>\n",
       "      <th>delay_hours</th>\n",
       "      <th>bl_or_awb_no</th>\n",
       "      <th>container_no</th>\n",
       "      <th>documents</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [consignment_id, created_at, origin_country, destination_country, origin_port, destination_port, shipment_mode, commodity, hs_code, gross_weight_kg, declared_value_usd, exporter_profile, doc_completeness_score, missing_docs_proxy, doc_amendments, congestion_index, is_weekend_created, customs_release_hours, terminal_dwell_hours, sla_hours, total_processing_hours, delayed_flag, delay_hours, bl_or_awb_no, container_no, documents, events]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate rows (exclude list-like/unhashable columns)\n",
    "# These columns are likely lists/dicts per the data dictionary and cannot be hashed\n",
    "hashable_cols = [c for c in df.columns if c not in [\"documents\", \"events\"]]\n",
    "\n",
    "print(f\"Number of duplicate rows (on hashable columns): {df.duplicated(subset=hashable_cols).sum()}\")\n",
    "print(f\"\\nTotal duplicate rows (including first occurrence, on hashable columns): {df.duplicated(subset=hashable_cols, keep=False).sum()}\")\n",
    "\n",
    "df[df.duplicated(subset=hashable_cols, keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "consignment_id            5000\n",
       "created_at                4999\n",
       "origin_country               5\n",
       "destination_country          7\n",
       "origin_port                  8\n",
       "destination_port            14\n",
       "shipment_mode                2\n",
       "commodity                    8\n",
       "hs_code                      6\n",
       "gross_weight_kg           4888\n",
       "declared_value_usd        4998\n",
       "exporter_profile             3\n",
       "doc_completeness_score     250\n",
       "missing_docs_proxy           4\n",
       "doc_amendments               6\n",
       "congestion_index           910\n",
       "is_weekend_created           2\n",
       "customs_release_hours     3236\n",
       "terminal_dwell_hours      3740\n",
       "sla_hours                    3\n",
       "total_processing_hours    4159\n",
       "delayed_flag                 2\n",
       "delay_hours               1860\n",
       "bl_or_awb_no              5000\n",
       "container_no              2309\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique values, ignoring unhashable list-like columns\n",
    "\n",
    "unique_counts = df[hashable_cols].nunique()\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check delayed consignments per shipment mode and plot\n",
    "\n",
    "# Group by shipment mode and calculate delayed consignments\n",
    "delayed_consignments = df.groupby('shipment_mode')['delayed_flag'].mean()\n",
    "\n",
    "# Plot the results\n",
    "delayed_consignments.plot(kind='bar', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chack delayed consignments per origin and plot\n",
    "\n",
    "# Group by origin and calculate delayed consignments\n",
    "delayed_consignments_origin = df.groupby('origin_country')['delayed_flag'].mean()\n",
    "\n",
    "# Plot the results\n",
    "delayed_consignments_origin.plot(kind='bar', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation between delayed consignments and all other features\n",
    "# Remove identifiers and non-numeric columns\n",
    "\n",
    "# Identify columns to exclude (identifiers and non-numeric)\n",
    "identifiers = ['consignment_id', 'bl_or_awb_no', 'container_no', 'documents', 'events']\n",
    "non_numeric = ['created_at', 'origin_country', 'destination_country', 'origin_port', \n",
    "               'destination_port', 'shipment_mode', 'commodity', 'exporter_profile']\n",
    "\n",
    "# Select only numeric columns (excluding identifiers)\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in identifiers]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap of Numeric Features (Excluding Identifiers)', \n",
    "          fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analytics - Pre-Modeling Analysis\n",
    "\n",
    "This section provides comprehensive descriptive analytics following a structured plan to understand delay patterns before building predictive models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Delay Landscape (Baseline Sanity Check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall delay rate\n",
    "total_consignments = len(df)\n",
    "delayed_count = df['delayed_flag'].sum()\n",
    "delay_rate = (delayed_count / total_consignments) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL DELAY LANDSCAPE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Consignments: {total_consignments:,}\")\n",
    "print(f\"Delayed Consignments: {delayed_count:,}\")\n",
    "print(f\"Not Delayed: {total_consignments - delayed_count:,}\")\n",
    "print(f\"Delay Rate: {delay_rate:.2f}%\")\n",
    "print(f\"Non-Delay Rate: {100 - delay_rate:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Distribution statistics\n",
    "print(\"\\nTOTAL PROCESSING HOURS - Distribution Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "processing_stats = df['total_processing_hours'].describe()\n",
    "print(processing_stats)\n",
    "print(f\"\\nP75 (75th percentile): {df['total_processing_hours'].quantile(0.75):.2f} hours\")\n",
    "print(f\"P90 (90th percentile): {df['total_processing_hours'].quantile(0.90):.2f} hours\")\n",
    "print(f\"P95 (95th percentile): {df['total_processing_hours'].quantile(0.95):.2f} hours\")\n",
    "print(f\"P99 (99th percentile): {df['total_processing_hours'].quantile(0.99):.2f} hours\")\n",
    "\n",
    "print(\"\\nDELAY HOURS - Distribution Statistics (for delayed consignments only):\")\n",
    "print(\"-\" * 60)\n",
    "delayed_df = df[df['delayed_flag'] == 1]\n",
    "if len(delayed_df) > 0:\n",
    "    delay_stats = delayed_df['delay_hours'].describe()\n",
    "    print(delay_stats)\n",
    "    print(f\"\\nP75: {delayed_df['delay_hours'].quantile(0.75):.2f} hours\")\n",
    "    print(f\"P90: {delayed_df['delay_hours'].quantile(0.90):.2f} hours\")\n",
    "    print(f\"P95: {delayed_df['delay_hours'].quantile(0.95):.2f} hours\")\n",
    "else:\n",
    "    print(\"No delayed consignments found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for overall delay landscape\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Delay vs Non-Delay Pie Chart\n",
    "axes[0, 0].pie([delayed_count, total_consignments - delayed_count], \n",
    "                labels=['Delayed', 'Not Delayed'],\n",
    "                autopct='%1.1f%%',\n",
    "                startangle=90,\n",
    "                colors=['#ff6b6b', '#51cf66'])\n",
    "axes[0, 0].set_title('Overall Delay Rate', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Distribution of total_processing_hours\n",
    "axes[0, 1].hist(df['total_processing_hours'], bins=50, edgecolor='black', alpha=0.7, color='#4dabf7')\n",
    "axes[0, 1].axvline(df['total_processing_hours'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {df[\"total_processing_hours\"].mean():.2f}h')\n",
    "axes[0, 1].axvline(df['total_processing_hours'].median(), color='green', linestyle='--', \n",
    "                   label=f'Median: {df[\"total_processing_hours\"].median():.2f}h')\n",
    "axes[0, 1].set_xlabel('Total Processing Hours')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Total Processing Hours', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Distribution of delay_hours (for delayed consignments only)\n",
    "if len(delayed_df) > 0:\n",
    "    axes[1, 0].hist(delayed_df['delay_hours'], bins=50, edgecolor='black', alpha=0.7, color='#ff6b6b')\n",
    "    axes[1, 0].axvline(delayed_df['delay_hours'].mean(), color='red', linestyle='--', \n",
    "                       label=f'Mean: {delayed_df[\"delay_hours\"].mean():.2f}h')\n",
    "    axes[1, 0].axvline(delayed_df['delay_hours'].median(), color='green', linestyle='--', \n",
    "                       label=f'Median: {delayed_df[\"delay_hours\"].median():.2f}h')\n",
    "    axes[1, 0].set_xlabel('Delay Hours')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Distribution of Delay Hours (Delayed Consignments Only)', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No delayed consignments', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Distribution of Delay Hours', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Box plot showing outliers\n",
    "box_data = [df[df['delayed_flag'] == 0]['total_processing_hours'], \n",
    "            df[df['delayed_flag'] == 1]['total_processing_hours']]\n",
    "axes[1, 1].boxplot(box_data, labels=['Not Delayed', 'Delayed'], patch_artist=True)\n",
    "axes[1, 1].set_ylabel('Total Processing Hours')\n",
    "axes[1, 1].set_title('Processing Hours: Delayed vs Not Delayed', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for long-tail behavior\n",
    "print(\"\\nLONG-TAIL BEHAVIOR ANALYSIS:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Consignments exceeding P95 ({df['total_processing_hours'].quantile(0.95):.2f}h): \"\n",
    "      f\"{(df['total_processing_hours'] > df['total_processing_hours'].quantile(0.95)).sum()}\")\n",
    "print(f\"Consignments exceeding P99 ({df['total_processing_hours'].quantile(0.99):.2f}h): \"\n",
    "      f\"{(df['total_processing_hours'] > df['total_processing_hours'].quantile(0.99)).sum()}\")\n",
    "print(f\"Maximum processing time: {df['total_processing_hours'].max():.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Delays by Origin Country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze delays by origin country\n",
    "origin_analysis = df.groupby('origin_country').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean',\n",
    "    'customs_release_hours': 'median',\n",
    "    'terminal_dwell_hours': 'median'\n",
    "}).round(2)\n",
    "\n",
    "origin_analysis.columns = ['Total_Consignments', 'Delayed_Count', 'Delay_Rate', \n",
    "                           'Avg_Processing_Hours', 'Median_Customs_Hours', \n",
    "                           'Median_Dwell_Hours']\n",
    "origin_analysis['Delay_Rate_Pct'] = (origin_analysis['Delay_Rate'] * 100).round(2)\n",
    "origin_analysis = origin_analysis.sort_values('Delay_Rate', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DELAYS BY ORIGIN COUNTRY\")\n",
    "print(\"=\" * 80)\n",
    "print(origin_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for origin country analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. Delay rate by origin country\n",
    "origin_delay_rate = origin_analysis.sort_values('Delay_Rate', ascending=True)\n",
    "axes[0, 0].barh(range(len(origin_delay_rate)), origin_delay_rate['Delay_Rate_Pct'], \n",
    "                color='#ff6b6b', alpha=0.7)\n",
    "axes[0, 0].set_yticks(range(len(origin_delay_rate)))\n",
    "axes[0, 0].set_yticklabels(origin_delay_rate.index)\n",
    "axes[0, 0].set_xlabel('Delay Rate (%)')\n",
    "axes[0, 0].set_title('Delay Rate by Origin Country', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Average processing hours by origin country\n",
    "origin_avg_processing = origin_analysis.sort_values('Avg_Processing_Hours', ascending=True)\n",
    "axes[0, 1].barh(range(len(origin_avg_processing)), origin_avg_processing['Avg_Processing_Hours'], \n",
    "                color='#4dabf7', alpha=0.7)\n",
    "axes[0, 1].set_yticks(range(len(origin_avg_processing)))\n",
    "axes[0, 1].set_yticklabels(origin_avg_processing.index)\n",
    "axes[0, 1].set_xlabel('Average Processing Hours')\n",
    "axes[0, 1].set_title('Average Processing Time by Origin Country', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Median customs release hours by origin country\n",
    "origin_customs = origin_analysis.sort_values('Median_Customs_Hours', ascending=True)\n",
    "axes[1, 0].barh(range(len(origin_customs)), origin_customs['Median_Customs_Hours'], \n",
    "                color='#51cf66', alpha=0.7)\n",
    "axes[1, 0].set_yticks(range(len(origin_customs)))\n",
    "axes[1, 0].set_yticklabels(origin_customs.index)\n",
    "axes[1, 0].set_xlabel('Median Customs Release Hours')\n",
    "axes[1, 0].set_title('Median Customs Release Time by Origin Country', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Median terminal dwell hours by origin country\n",
    "origin_dwell = origin_analysis.sort_values('Median_Dwell_Hours', ascending=True)\n",
    "axes[1, 1].barh(range(len(origin_dwell)), origin_dwell['Median_Dwell_Hours'], \n",
    "                color='#ffd43b', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(origin_dwell)))\n",
    "axes[1, 1].set_yticklabels(origin_dwell.index)\n",
    "axes[1, 1].set_xlabel('Median Terminal Dwell Hours')\n",
    "axes[1, 1].set_title('Median Terminal Dwell Time by Origin Country', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top 5 origin countries with highest delay rates\n",
    "print(\"\\nTOP 5 ORIGIN COUNTRIES WITH HIGHEST DELAY RATES:\")\n",
    "print(\"-\" * 80)\n",
    "print(origin_analysis.head(5)[['Total_Consignments', 'Delayed_Count', 'Delay_Rate_Pct', \n",
    "                                'Avg_Processing_Hours']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Delays by Destination Country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze delays by destination country\n",
    "dest_analysis = df.groupby('destination_country').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean',\n",
    "    'terminal_dwell_hours': 'median',\n",
    "    'shipment_mode': lambda x: x.value_counts().to_dict()\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "dest_analysis.columns = ['Total_Consignments', 'Delayed_Count', 'Delay_Rate', \n",
    "                         'Avg_Processing_Hours', 'Median_Dwell_Hours', 'Shipment_Mode_Dict']\n",
    "\n",
    "# Calculate AIR vs SEA split\n",
    "dest_analysis['AIR_Count'] = dest_analysis['Shipment_Mode_Dict'].apply(\n",
    "    lambda x: x.get('AIR', 0) if isinstance(x, dict) else 0\n",
    ")\n",
    "dest_analysis['SEA_Count'] = dest_analysis['Shipment_Mode_Dict'].apply(\n",
    "    lambda x: x.get('SEA', 0) if isinstance(x, dict) else 0\n",
    ")\n",
    "dest_analysis['AIR_Pct'] = (dest_analysis['AIR_Count'] / dest_analysis['Total_Consignments'] * 100).round(2)\n",
    "dest_analysis['SEA_Pct'] = (dest_analysis['SEA_Count'] / dest_analysis['Total_Consignments'] * 100).round(2)\n",
    "\n",
    "dest_analysis['Delay_Rate_Pct'] = (dest_analysis['Delay_Rate'] * 100).round(2)\n",
    "dest_analysis = dest_analysis.sort_values('Delay_Rate', ascending=False)\n",
    "\n",
    "# Drop the dict column for display\n",
    "dest_display = dest_analysis.drop('Shipment_Mode_Dict', axis=1)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"DELAYS BY DESTINATION COUNTRY\")\n",
    "print(\"=\" * 100)\n",
    "print(dest_display.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for destination country analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. Delay rate by destination country\n",
    "dest_delay_rate = dest_analysis.sort_values('Delay_Rate', ascending=True)\n",
    "axes[0, 0].barh(range(len(dest_delay_rate)), dest_delay_rate['Delay_Rate_Pct'], \n",
    "                color='#ff6b6b', alpha=0.7)\n",
    "axes[0, 0].set_yticks(range(len(dest_delay_rate)))\n",
    "axes[0, 0].set_yticklabels(dest_delay_rate.index)\n",
    "axes[0, 0].set_xlabel('Delay Rate (%)')\n",
    "axes[0, 0].set_title('Delay Rate by Destination Country', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Average processing hours by destination country\n",
    "dest_avg_processing = dest_analysis.sort_values('Avg_Processing_Hours', ascending=True)\n",
    "axes[0, 1].barh(range(len(dest_avg_processing)), dest_avg_processing['Avg_Processing_Hours'], \n",
    "                color='#4dabf7', alpha=0.7)\n",
    "axes[0, 1].set_yticks(range(len(dest_avg_processing)))\n",
    "axes[0, 1].set_yticklabels(dest_avg_processing.index)\n",
    "axes[0, 1].set_xlabel('Average Processing Hours')\n",
    "axes[0, 1].set_title('Average Processing Time by Destination Country', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Median dwell time by destination country\n",
    "dest_dwell = dest_analysis.sort_values('Median_Dwell_Hours', ascending=True)\n",
    "axes[1, 0].barh(range(len(dest_dwell)), dest_dwell['Median_Dwell_Hours'], \n",
    "                color='#ffd43b', alpha=0.7)\n",
    "axes[1, 0].set_yticks(range(len(dest_dwell)))\n",
    "axes[1, 0].set_yticklabels(dest_dwell.index)\n",
    "axes[1, 0].set_xlabel('Median Terminal Dwell Hours')\n",
    "axes[1, 0].set_title('Median Terminal Dwell Time by Destination Country', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 4. AIR vs SEA split by destination\n",
    "x_pos = range(len(dest_analysis))\n",
    "width = 0.35\n",
    "dest_sorted = dest_analysis.sort_values('Total_Consignments', ascending=True)\n",
    "axes[1, 1].barh([x - width/2 for x in x_pos], dest_sorted['AIR_Pct'], width, \n",
    "                label='AIR', color='#51cf66', alpha=0.7)\n",
    "axes[1, 1].barh([x + width/2 for x in x_pos], dest_sorted['SEA_Pct'], width, \n",
    "                label='SEA', color='#4dabf7', alpha=0.7)\n",
    "axes[1, 1].set_yticks(x_pos)\n",
    "axes[1, 1].set_yticklabels(dest_sorted.index)\n",
    "axes[1, 1].set_xlabel('Percentage of Consignments')\n",
    "axes[1, 1].set_title('AIR vs SEA Split by Destination Country', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare AIR vs SEA delay rates within each destination\n",
    "print(\"\\nAIR vs SEA DELAY RATES BY DESTINATION:\")\n",
    "print(\"-\" * 80)\n",
    "air_sea_dest = df.groupby(['destination_country', 'shipment_mode'])['delayed_flag'].agg(['count', 'sum', 'mean']).round(3)\n",
    "air_sea_dest.columns = ['Count', 'Delayed_Count', 'Delay_Rate']\n",
    "air_sea_dest['Delay_Rate_Pct'] = (air_sea_dest['Delay_Rate'] * 100).round(2)\n",
    "print(air_sea_dest.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Port / Airport Performance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze origin port performance\n",
    "origin_port_analysis = df.groupby('origin_port').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean',\n",
    "    'customs_release_hours': 'mean',\n",
    "    'terminal_dwell_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "origin_port_analysis.columns = ['Total_Consignments', 'Delayed_Count', 'Delay_Rate', \n",
    "                                'Avg_Processing_Hours', 'Avg_Customs_Hours', \n",
    "                                'Avg_Dwell_Hours']\n",
    "origin_port_analysis['Delay_Rate_Pct'] = (origin_port_analysis['Delay_Rate'] * 100).round(2)\n",
    "origin_port_analysis = origin_port_analysis.sort_values('Delay_Rate', ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"ORIGIN PORT PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "print(origin_port_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze destination port performance\n",
    "dest_port_analysis = df.groupby('destination_port').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean',\n",
    "    'terminal_dwell_hours': 'mean',\n",
    "    'customs_release_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "dest_port_analysis.columns = ['Total_Consignments', 'Delayed_Count', 'Delay_Rate', \n",
    "                               'Avg_Processing_Hours', 'Avg_Dwell_Hours', \n",
    "                               'Avg_Customs_Hours']\n",
    "dest_port_analysis['Delay_Rate_Pct'] = (dest_port_analysis['Delay_Rate'] * 100).round(2)\n",
    "dest_port_analysis = dest_port_analysis.sort_values('Delay_Rate', ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"DESTINATION PORT PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "print(dest_port_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for port performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. Origin port delay rates (top 10)\n",
    "top_origin_ports = origin_port_analysis.head(10).sort_values('Delay_Rate', ascending=True)\n",
    "axes[0, 0].barh(range(len(top_origin_ports)), top_origin_ports['Delay_Rate_Pct'], \n",
    "                color='#ff6b6b', alpha=0.7)\n",
    "axes[0, 0].set_yticks(range(len(top_origin_ports)))\n",
    "axes[0, 0].set_yticklabels(top_origin_ports.index)\n",
    "axes[0, 0].set_xlabel('Delay Rate (%)')\n",
    "axes[0, 0].set_title('Top 10 Origin Ports by Delay Rate', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Destination port delay rates (top 10)\n",
    "top_dest_ports = dest_port_analysis.head(10).sort_values('Delay_Rate', ascending=True)\n",
    "axes[0, 1].barh(range(len(top_dest_ports)), top_dest_ports['Delay_Rate_Pct'], \n",
    "                color='#ff6b6b', alpha=0.7)\n",
    "axes[0, 1].set_yticks(range(len(top_dest_ports)))\n",
    "axes[0, 1].set_yticklabels(top_dest_ports.index)\n",
    "axes[0, 1].set_xlabel('Delay Rate (%)')\n",
    "axes[0, 1].set_title('Top 10 Destination Ports by Delay Rate', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Scatter: Customs vs Dwell time for origin ports (identify bottlenecks)\n",
    "axes[1, 0].scatter(origin_port_analysis['Avg_Customs_Hours'], \n",
    "                   origin_port_analysis['Avg_Dwell_Hours'],\n",
    "                   s=origin_port_analysis['Total_Consignments']*2, \n",
    "                   alpha=0.6, c=origin_port_analysis['Delay_Rate_Pct'], \n",
    "                   cmap='Reds', edgecolors='black', linewidth=0.5)\n",
    "axes[1, 0].set_xlabel('Average Customs Release Hours')\n",
    "axes[1, 0].set_ylabel('Average Terminal Dwell Hours')\n",
    "axes[1, 0].set_title('Origin Ports: Customs vs Dwell Time\\n(Bubble size = Volume, Color = Delay Rate)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "plt.colorbar(axes[1, 0].collections[0], ax=axes[1, 0], label='Delay Rate (%)')\n",
    "\n",
    "# 4. Scatter: Customs vs Dwell time for destination ports\n",
    "axes[1, 1].scatter(dest_port_analysis['Avg_Customs_Hours'], \n",
    "                   dest_port_analysis['Avg_Dwell_Hours'],\n",
    "                   s=dest_port_analysis['Total_Consignments']*2, \n",
    "                   alpha=0.6, c=dest_port_analysis['Delay_Rate_Pct'], \n",
    "                   cmap='Reds', edgecolors='black', linewidth=0.5)\n",
    "axes[1, 1].set_xlabel('Average Customs Release Hours')\n",
    "axes[1, 1].set_ylabel('Average Terminal Dwell Hours')\n",
    "axes[1, 1].set_title('Destination Ports: Customs vs Dwell Time\\n(Bubble size = Volume, Color = Delay Rate)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1], label='Delay Rate (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify bottleneck patterns\n",
    "print(\"\\nPORT BOTTLENECK ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nOrigin Ports with High Dwell but Normal Customs (Terminal Congestion):\")\n",
    "high_dwell_origin = origin_port_analysis[\n",
    "    (origin_port_analysis['Avg_Dwell_Hours'] > origin_port_analysis['Avg_Dwell_Hours'].quantile(0.75)) &\n",
    "    (origin_port_analysis['Avg_Customs_Hours'] <= origin_port_analysis['Avg_Customs_Hours'].median())\n",
    "].sort_values('Avg_Dwell_Hours', ascending=False)\n",
    "print(high_dwell_origin[['Avg_Dwell_Hours', 'Avg_Customs_Hours', 'Delay_Rate_Pct']].to_string())\n",
    "\n",
    "print(\"\\nOrigin Ports with Slow Customs but Normal Dwell (Regulatory Bottleneck):\")\n",
    "slow_customs_origin = origin_port_analysis[\n",
    "    (origin_port_analysis['Avg_Customs_Hours'] > origin_port_analysis['Avg_Customs_Hours'].quantile(0.75)) &\n",
    "    (origin_port_analysis['Avg_Dwell_Hours'] <= origin_port_analysis['Avg_Dwell_Hours'].median())\n",
    "].sort_values('Avg_Customs_Hours', ascending=False)\n",
    "print(slow_customs_origin[['Avg_Customs_Hours', 'Avg_Dwell_Hours', 'Delay_Rate_Pct']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time-of-Day Effects (Day vs Night)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time features from created_at\n",
    "df['hour_of_day'] = df['created_at'].dt.hour\n",
    "df['day_of_week'] = df['created_at'].dt.day_name()\n",
    "\n",
    "# Create time buckets\n",
    "def get_time_bucket(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Night (00:00-05:59)'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning (06:00-11:59)'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon (12:00-17:59)'\n",
    "    else:\n",
    "        return 'Evening (18:00-23:59)'\n",
    "\n",
    "df['time_of_day_bucket'] = df['hour_of_day'].apply(get_time_bucket)\n",
    "\n",
    "# Analyze time-of-day effects\n",
    "time_analysis = df.groupby('time_of_day_bucket').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean',\n",
    "    'customs_release_hours': 'mean',\n",
    "    'terminal_dwell_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "time_analysis.columns = ['Total_Consignments', 'Delayed_Count', 'Delay_Rate', \n",
    "                         'Avg_Processing_Hours', 'Avg_Customs_Hours', 'Avg_Dwell_Hours']\n",
    "time_analysis['Delay_Rate_Pct'] = (time_analysis['Delay_Rate'] * 100).round(2)\n",
    "\n",
    "# Reorder for logical display\n",
    "time_order = ['Night (00:00-05:59)', 'Morning (06:00-11:59)', \n",
    "              'Afternoon (12:00-17:59)', 'Evening (18:00-23:59)']\n",
    "time_analysis = time_analysis.reindex(time_order)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TIME-OF-DAY EFFECTS ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "print(time_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for time-of-day effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Delay rate by time bucket\n",
    "axes[0, 0].bar(range(len(time_analysis)), time_analysis['Delay_Rate_Pct'], \n",
    "               color=['#2c3e50', '#3498db', '#e74c3c', '#f39c12'], alpha=0.7)\n",
    "axes[0, 0].set_xticks(range(len(time_analysis)))\n",
    "axes[0, 0].set_xticklabels(time_analysis.index, rotation=15, ha='right')\n",
    "axes[0, 0].set_ylabel('Delay Rate (%)')\n",
    "axes[0, 0].set_title('Delay Rate by Time of Day', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Average processing hours by time bucket\n",
    "axes[0, 1].bar(range(len(time_analysis)), time_analysis['Avg_Processing_Hours'], \n",
    "               color=['#2c3e50', '#3498db', '#e74c3c', '#f39c12'], alpha=0.7)\n",
    "axes[0, 1].set_xticks(range(len(time_analysis)))\n",
    "axes[0, 1].set_xticklabels(time_analysis.index, rotation=15, ha='right')\n",
    "axes[0, 1].set_ylabel('Average Processing Hours')\n",
    "axes[0, 1].set_title('Average Processing Time by Time of Day', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Distribution of consignments by hour of day\n",
    "hour_dist = df['hour_of_day'].value_counts().sort_index()\n",
    "axes[1, 0].bar(hour_dist.index, hour_dist.values, color='#4dabf7', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('Number of Consignments')\n",
    "axes[1, 0].set_title('Distribution of Consignments by Hour of Day', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticks(range(0, 24, 2))\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Delay rate by hour of day\n",
    "hour_delay = df.groupby('hour_of_day')['delayed_flag'].mean() * 100\n",
    "axes[1, 1].plot(hour_delay.index, hour_delay.values, marker='o', linewidth=2, \n",
    "                markersize=6, color='#e74c3c')\n",
    "axes[1, 1].set_xlabel('Hour of Day')\n",
    "axes[1, 1].set_ylabel('Delay Rate (%)')\n",
    "axes[1, 1].set_title('Delay Rate by Hour of Day', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(range(0, 24, 2))\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].axhline(y=df['delayed_flag'].mean() * 100, color='green', \n",
    "                   linestyle='--', label=f'Overall Avg: {df[\"delayed_flag\"].mean()*100:.2f}%')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Night vs Day comparison\n",
    "print(\"\\nNIGHT vs DAY COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "night_df = df[df['time_of_day_bucket'] == 'Night (00:00-05:59)']\n",
    "day_df = df[df['time_of_day_bucket'].isin(['Morning (06:00-11:59)', \n",
    "                                           'Afternoon (12:00-17:59)', \n",
    "                                           'Evening (18:00-23:59)'])]\n",
    "\n",
    "print(f\"Night Consignments:\")\n",
    "print(f\"  Count: {len(night_df):,}\")\n",
    "print(f\"  Delay Rate: {night_df['delayed_flag'].mean()*100:.2f}%\")\n",
    "print(f\"  Avg Processing Hours: {night_df['total_processing_hours'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nDay Consignments:\")\n",
    "print(f\"  Count: {len(day_df):,}\")\n",
    "print(f\"  Delay Rate: {day_df['delayed_flag'].mean()*100:.2f}%\")\n",
    "print(f\"  Avg Processing Hours: {day_df['total_processing_hours'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Day-of-Week Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze day-of-week effects\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df['day_of_week'] = pd.Categorical(df['day_of_week'], categories=weekday_order, ordered=True)\n",
    "\n",
    "weekday_analysis = df.groupby('day_of_week', observed=True).agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean',\n",
    "    'customs_release_hours': 'mean',\n",
    "    'terminal_dwell_hours': 'mean',\n",
    "    'is_weekend_created': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "weekday_analysis.columns = ['Total_Consignments', 'Delayed_Count', 'Delay_Rate', \n",
    "                             'Avg_Processing_Hours', 'Avg_Customs_Hours', \n",
    "                             'Avg_Dwell_Hours', 'Weekend_Created_Count']\n",
    "weekday_analysis['Delay_Rate_Pct'] = (weekday_analysis['Delay_Rate'] * 100).round(2)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"DAY-OF-WEEK ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "print(weekday_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for day-of-week analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Delay rate by weekday\n",
    "axes[0, 0].bar(range(len(weekday_analysis)), weekday_analysis['Delay_Rate_Pct'], \n",
    "               color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xticks(range(len(weekday_analysis)))\n",
    "axes[0, 0].set_xticklabels(weekday_analysis.index, rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('Delay Rate (%)')\n",
    "axes[0, 0].set_title('Delay Rate by Day of Week', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axhline(y=df['delayed_flag'].mean() * 100, color='green', \n",
    "                   linestyle='--', label=f'Overall Avg: {df[\"delayed_flag\"].mean()*100:.2f}%')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Average processing time by weekday\n",
    "axes[0, 1].bar(range(len(weekday_analysis)), weekday_analysis['Avg_Processing_Hours'], \n",
    "               color='#3498db', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xticks(range(len(weekday_analysis)))\n",
    "axes[0, 1].set_xticklabels(weekday_analysis.index, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Average Processing Hours')\n",
    "axes[0, 1].set_title('Average Processing Time by Day of Week', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axhline(y=df['total_processing_hours'].mean(), color='green', \n",
    "                   linestyle='--', label=f'Overall Avg: {df[\"total_processing_hours\"].mean():.2f}h')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Volume by weekday\n",
    "axes[1, 0].bar(range(len(weekday_analysis)), weekday_analysis['Total_Consignments'], \n",
    "               color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xticks(range(len(weekday_analysis)))\n",
    "axes[1, 0].set_xticklabels(weekday_analysis.index, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Number of Consignments')\n",
    "axes[1, 0].set_title('Volume by Day of Week', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Weekend vs Weekday comparison\n",
    "weekend_delay = df[df['is_weekend_created'] == 1]['delayed_flag'].mean() * 100\n",
    "weekday_delay = df[df['is_weekend_created'] == 0]['delayed_flag'].mean() * 100\n",
    "\n",
    "weekend_processing = df[df['is_weekend_created'] == 1]['total_processing_hours'].mean()\n",
    "weekday_processing = df[df['is_weekend_created'] == 0]['total_processing_hours'].mean()\n",
    "\n",
    "comparison_data = {\n",
    "    'Weekend Created': [weekend_delay, weekend_processing],\n",
    "    'Weekday Created': [weekday_delay, weekday_processing]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_data, index=['Delay Rate (%)', 'Avg Processing (hours)'])\n",
    "\n",
    "x = range(len(comparison_df))\n",
    "width = 0.35\n",
    "axes[1, 1].bar([i - width/2 for i in x], comparison_df['Weekend Created'], width, \n",
    "               label='Weekend Created', color='#e74c3c', alpha=0.7)\n",
    "axes[1, 1].bar([i + width/2 for i in x], comparison_df['Weekday Created'], width, \n",
    "               label='Weekday Created', color='#3498db', alpha=0.7)\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(comparison_df.index)\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].set_title('Weekend vs Weekday Created Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWEEKEND vs WEEKDAY SUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Weekend Created Consignments: {df['is_weekend_created'].sum():,}\")\n",
    "print(f\"  Delay Rate: {weekend_delay:.2f}%\")\n",
    "print(f\"  Avg Processing: {weekend_processing:.2f} hours\")\n",
    "print(f\"\\nWeekday Created Consignments: {(df['is_weekend_created'] == 0).sum():,}\")\n",
    "print(f\"  Delay Rate: {weekday_delay:.2f}%\")\n",
    "print(f\"  Avg Processing: {weekday_processing:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Commodity-Level Delay Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze commodity-level delay patterns\n",
    "commodity_analysis = df.groupby('commodity').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean',\n",
    "    'customs_release_hours': 'mean',\n",
    "    'terminal_dwell_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "commodity_analysis.columns = ['Total_Consignments', 'Delayed_Count', 'Delay_Rate', \n",
    "                               'Avg_Processing_Hours', 'Avg_Customs_Hours', \n",
    "                               'Avg_Dwell_Hours']\n",
    "commodity_analysis['Delay_Rate_Pct'] = (commodity_analysis['Delay_Rate'] * 100).round(2)\n",
    "commodity_analysis = commodity_analysis.sort_values('Delay_Rate', ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMMODITY-LEVEL DELAY PATTERNS\")\n",
    "print(\"=\" * 100)\n",
    "print(commodity_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIR vs SEA comparison per commodity\n",
    "commodity_mode_analysis = df.groupby(['commodity', 'shipment_mode']).agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "commodity_mode_analysis.columns = ['Count', 'Delayed_Count', 'Delay_Rate', 'Avg_Processing_Hours']\n",
    "commodity_mode_analysis['Delay_Rate_Pct'] = (commodity_mode_analysis['Delay_Rate'] * 100).round(2)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMMODITY DELAY PATTERNS BY SHIPMENT MODE (AIR vs SEA)\")\n",
    "print(\"=\" * 100)\n",
    "print(commodity_mode_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for commodity analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. Delay rate by commodity\n",
    "commodity_delay = commodity_analysis.sort_values('Delay_Rate', ascending=True)\n",
    "axes[0, 0].barh(range(len(commodity_delay)), commodity_delay['Delay_Rate_Pct'], \n",
    "                color='#e74c3c', alpha=0.7)\n",
    "axes[0, 0].set_yticks(range(len(commodity_delay)))\n",
    "axes[0, 0].set_yticklabels(commodity_delay.index)\n",
    "axes[0, 0].set_xlabel('Delay Rate (%)')\n",
    "axes[0, 0].set_title('Delay Rate by Commodity', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Average processing time by commodity\n",
    "commodity_processing = commodity_analysis.sort_values('Avg_Processing_Hours', ascending=True)\n",
    "axes[0, 1].barh(range(len(commodity_processing)), commodity_processing['Avg_Processing_Hours'], \n",
    "                color='#3498db', alpha=0.7)\n",
    "axes[0, 1].set_yticks(range(len(commodity_processing)))\n",
    "axes[0, 1].set_yticklabels(commodity_processing.index)\n",
    "axes[0, 1].set_xlabel('Average Processing Hours')\n",
    "axes[0, 1].set_title('Average Processing Time by Commodity', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 3. AIR vs SEA delay rates by commodity\n",
    "commodities = commodity_mode_analysis.index.get_level_values(0).unique()\n",
    "air_delays = []\n",
    "sea_delays = []\n",
    "commodity_labels = []\n",
    "\n",
    "for commodity in commodities:\n",
    "    if (commodity, 'AIR') in commodity_mode_analysis.index:\n",
    "        air_delays.append(commodity_mode_analysis.loc[(commodity, 'AIR'), 'Delay_Rate_Pct'])\n",
    "    else:\n",
    "        air_delays.append(0)\n",
    "    \n",
    "    if (commodity, 'SEA') in commodity_mode_analysis.index:\n",
    "        sea_delays.append(commodity_mode_analysis.loc[(commodity, 'SEA'), 'Delay_Rate_Pct'])\n",
    "    else:\n",
    "        sea_delays.append(0)\n",
    "    \n",
    "    commodity_labels.append(commodity)\n",
    "\n",
    "x = range(len(commodities))\n",
    "width = 0.35\n",
    "axes[1, 0].bar([i - width/2 for i in x], air_delays, width, \n",
    "               label='AIR', color='#51cf66', alpha=0.7)\n",
    "axes[1, 0].bar([i + width/2 for i in x], sea_delays, width, \n",
    "               label='SEA', color='#4dabf7', alpha=0.7)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(commodity_labels, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Delay Rate (%)')\n",
    "axes[1, 0].set_title('AIR vs SEA Delay Rates by Commodity', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Volume by commodity\n",
    "axes[1, 1].bar(range(len(commodity_analysis)), commodity_analysis['Total_Consignments'], \n",
    "               color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xticks(range(len(commodity_analysis)))\n",
    "axes[1, 1].set_xticklabels(commodity_analysis.index, rotation=45, ha='right')\n",
    "axes[1, 1].set_ylabel('Number of Consignments')\n",
    "axes[1, 1].set_title('Volume by Commodity', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top commodities by delay rate\n",
    "print(\"\\nTOP 5 COMMODITIES BY DELAY RATE:\")\n",
    "print(\"-\" * 80)\n",
    "print(commodity_analysis.head(5)[['Total_Consignments', 'Delayed_Count', 'Delay_Rate_Pct', \n",
    "                                    'Avg_Processing_Hours']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Countries with Longest Processing Times (Not Just Delays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank countries by mean processing time (separate for origin and destination)\n",
    "origin_processing_rank = df.groupby('origin_country')['total_processing_hours'].mean().sort_values(ascending=False)\n",
    "dest_processing_rank = df.groupby('destination_country')['total_processing_hours'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Rank countries by delay rate\n",
    "origin_delay_rank = df.groupby('origin_country')['delayed_flag'].mean().sort_values(ascending=False) * 100\n",
    "dest_delay_rank = df.groupby('destination_country')['delayed_flag'].mean().sort_values(ascending=False) * 100\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COUNTRIES RANKED BY PROCESSING TIME vs DELAY RATE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nORIGIN COUNTRIES - Ranked by Mean Processing Time:\")\n",
    "print(\"-\" * 100)\n",
    "origin_processing_df = pd.DataFrame({\n",
    "    'Mean_Processing_Hours': origin_processing_rank,\n",
    "    'Delay_Rate_Pct': origin_delay_rank[origin_processing_rank.index]\n",
    "}).round(2)\n",
    "print(origin_processing_df.to_string())\n",
    "\n",
    "print(\"\\n\\nORIGIN COUNTRIES - Ranked by Delay Rate:\")\n",
    "print(\"-\" * 100)\n",
    "origin_delay_df = pd.DataFrame({\n",
    "    'Delay_Rate_Pct': origin_delay_rank,\n",
    "    'Mean_Processing_Hours': origin_processing_rank[origin_delay_rank.index]\n",
    "}).round(2)\n",
    "print(origin_delay_df.to_string())\n",
    "\n",
    "print(\"\\n\\nDESTINATION COUNTRIES - Ranked by Mean Processing Time:\")\n",
    "print(\"-\" * 100)\n",
    "dest_processing_df = pd.DataFrame({\n",
    "    'Mean_Processing_Hours': dest_processing_rank,\n",
    "    'Delay_Rate_Pct': dest_delay_rank[dest_processing_rank.index]\n",
    "}).round(2)\n",
    "print(dest_processing_df.to_string())\n",
    "\n",
    "print(\"\\n\\nDESTINATION COUNTRIES - Ranked by Delay Rate:\")\n",
    "print(\"-\" * 100)\n",
    "dest_delay_df = pd.DataFrame({\n",
    "    'Delay_Rate_Pct': dest_delay_rank,\n",
    "    'Mean_Processing_Hours': dest_processing_rank[dest_delay_rank.index]\n",
    "}).round(2)\n",
    "print(dest_delay_df.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations comparing processing time vs delay rate\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. Origin countries: Processing time vs Delay rate scatter\n",
    "axes[0, 0].scatter(origin_processing_rank.values, origin_delay_rank[origin_processing_rank.index].values,\n",
    "                   s=100, alpha=0.6, color='#e74c3c', edgecolors='black', linewidth=0.5)\n",
    "for country in origin_processing_rank.head(5).index:\n",
    "    axes[0, 0].annotate(country, \n",
    "                       (origin_processing_rank[country], origin_delay_rank[country]),\n",
    "                       fontsize=8, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Mean Processing Hours')\n",
    "axes[0, 0].set_ylabel('Delay Rate (%)')\n",
    "axes[0, 0].set_title('Origin Countries: Processing Time vs Delay Rate', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Destination countries: Processing time vs Delay rate scatter\n",
    "axes[0, 1].scatter(dest_processing_rank.values, dest_delay_rank[dest_processing_rank.index].values,\n",
    "                   s=100, alpha=0.6, color='#3498db', edgecolors='black', linewidth=0.5)\n",
    "for country in dest_processing_rank.head(5).index:\n",
    "    axes[0, 1].annotate(country, \n",
    "                       (dest_processing_rank[country], dest_delay_rank[country]),\n",
    "                       fontsize=8, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Mean Processing Hours')\n",
    "axes[0, 1].set_ylabel('Delay Rate (%)')\n",
    "axes[0, 1].set_title('Destination Countries: Processing Time vs Delay Rate', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Top 10 origin countries by processing time\n",
    "top_origin_proc = origin_processing_rank.head(10).sort_values(ascending=True)\n",
    "axes[1, 0].barh(range(len(top_origin_proc)), top_origin_proc.values, \n",
    "                color='#e74c3c', alpha=0.7)\n",
    "axes[1, 0].set_yticks(range(len(top_origin_proc)))\n",
    "axes[1, 0].set_yticklabels(top_origin_proc.index)\n",
    "axes[1, 0].set_xlabel('Mean Processing Hours')\n",
    "axes[1, 0].set_title('Top 10 Origin Countries by Processing Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Top 10 destination countries by processing time\n",
    "top_dest_proc = dest_processing_rank.head(10).sort_values(ascending=True)\n",
    "axes[1, 1].barh(range(len(top_dest_proc)), top_dest_proc.values, \n",
    "                color='#3498db', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(top_dest_proc)))\n",
    "axes[1, 1].set_yticklabels(top_dest_proc.index)\n",
    "axes[1, 1].set_xlabel('Mean Processing Hours')\n",
    "axes[1, 1].set_title('Top 10 Destination Countries by Processing Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify countries with long processing but low delay rate (and vice versa)\n",
    "print(\"\\nKEY INSIGHTS - Processing Time vs Delay Rate:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nOrigin Countries with Long Processing but Low Delay Rate (Slow but Reliable):\")\n",
    "long_low_origin = origin_processing_df[\n",
    "    (origin_processing_df['Mean_Processing_Hours'] > origin_processing_df['Mean_Processing_Hours'].quantile(0.75)) &\n",
    "    (origin_processing_df['Delay_Rate_Pct'] < origin_processing_df['Delay_Rate_Pct'].median())\n",
    "].sort_values('Mean_Processing_Hours', ascending=False)\n",
    "print(long_low_origin.to_string())\n",
    "\n",
    "print(\"\\n\\nOrigin Countries with Fast Processing but High Delay Rate (Fast but Unreliable):\")\n",
    "fast_high_origin = origin_processing_df[\n",
    "    (origin_processing_df['Mean_Processing_Hours'] < origin_processing_df['Mean_Processing_Hours'].median()) &\n",
    "    (origin_processing_df['Delay_Rate_Pct'] > origin_processing_df['Delay_Rate_Pct'].quantile(0.75))\n",
    "].sort_values('Delay_Rate_Pct', ascending=False)\n",
    "print(fast_high_origin.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Early Red-Flag Analysis (Precursor Signals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze doc_completeness_score vs delay rate\n",
    "df['doc_completeness_bucket'] = pd.cut(df['doc_completeness_score'], \n",
    "                                       bins=[0, 0.5, 0.7, 0.9, 1.0], \n",
    "                                       labels=['Low (0-0.5)', 'Medium (0.5-0.7)', \n",
    "                                               'High (0.7-0.9)', 'Very High (0.9-1.0)'])\n",
    "\n",
    "doc_completeness_analysis = df.groupby('doc_completeness_bucket').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "doc_completeness_analysis.columns = ['Count', 'Delayed_Count', 'Delay_Rate', 'Avg_Processing_Hours']\n",
    "doc_completeness_analysis['Delay_Rate_Pct'] = (doc_completeness_analysis['Delay_Rate'] * 100).round(2)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"DOCUMENT COMPLETENESS SCORE vs DELAY RATE\")\n",
    "print(\"=\" * 100)\n",
    "print(doc_completeness_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze doc_amendments vs delay rate\n",
    "df['amendments_bucket'] = pd.cut(df['doc_amendments'], \n",
    "                                 bins=[-1, 0, 1, 2, 10], \n",
    "                                 labels=['0', '1', '2', '3+'])\n",
    "\n",
    "amendments_analysis = df.groupby('amendments_bucket').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "amendments_analysis.columns = ['Count', 'Delayed_Count', 'Delay_Rate', 'Avg_Processing_Hours']\n",
    "amendments_analysis['Delay_Rate_Pct'] = (amendments_analysis['Delay_Rate'] * 100).round(2)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"DOCUMENT AMENDMENTS vs DELAY RATE\")\n",
    "print(\"=\" * 100)\n",
    "print(amendments_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze congestion_index vs delay rate\n",
    "df['congestion_bucket'] = pd.cut(df['congestion_index'], \n",
    "                                 bins=[0, 0.3, 0.5, 0.7, 1.0], \n",
    "                                 labels=['Low (0-0.3)', 'Medium (0.3-0.5)', \n",
    "                                         'High (0.5-0.7)', 'Very High (0.7-1.0)'])\n",
    "\n",
    "congestion_analysis = df.groupby('congestion_bucket').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "congestion_analysis.columns = ['Count', 'Delayed_Count', 'Delay_Rate', 'Avg_Processing_Hours']\n",
    "congestion_analysis['Delay_Rate_Pct'] = (congestion_analysis['Delay_Rate'] * 100).round(2)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"CONGESTION INDEX vs DELAY RATE\")\n",
    "print(\"=\" * 100)\n",
    "print(congestion_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weight and value vs processing time\n",
    "df['weight_bucket'] = pd.qcut(df['gross_weight_kg'], q=4, \n",
    "                              labels=['Q1 (Lightest)', 'Q2', 'Q3', 'Q4 (Heaviest)'], \n",
    "                              duplicates='drop')\n",
    "df['value_bucket'] = pd.qcut(df['declared_value_usd'], q=4, \n",
    "                             labels=['Q1 (Lowest)', 'Q2', 'Q3', 'Q4 (Highest)'], \n",
    "                             duplicates='drop')\n",
    "\n",
    "weight_analysis = df.groupby('weight_bucket').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "weight_analysis.columns = ['Count', 'Delayed_Count', 'Delay_Rate', 'Avg_Processing_Hours']\n",
    "weight_analysis['Delay_Rate_Pct'] = (weight_analysis['Delay_Rate'] * 100).round(2)\n",
    "\n",
    "value_analysis = df.groupby('value_bucket').agg({\n",
    "    'delayed_flag': ['count', 'sum', 'mean'],\n",
    "    'total_processing_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "value_analysis.columns = ['Count', 'Delayed_Count', 'Delay_Rate', 'Avg_Processing_Hours']\n",
    "value_analysis['Delay_Rate_Pct'] = (value_analysis['Delay_Rate'] * 100).round(2)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"WEIGHT vs DELAY RATE\")\n",
    "print(\"=\" * 100)\n",
    "print(weight_analysis.to_string())\n",
    "\n",
    "print(\"\\n\\nVALUE vs DELAY RATE\")\n",
    "print(\"=\" * 100)\n",
    "print(value_analysis.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations for early red-flag analysis\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Document completeness vs delay rate\n",
    "axes[0, 0].bar(range(len(doc_completeness_analysis)), doc_completeness_analysis['Delay_Rate_Pct'], \n",
    "               color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xticks(range(len(doc_completeness_analysis)))\n",
    "axes[0, 0].set_xticklabels(doc_completeness_analysis.index, rotation=15, ha='right')\n",
    "axes[0, 0].set_ylabel('Delay Rate (%)')\n",
    "axes[0, 0].set_title('Delay Rate by Document Completeness Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Document amendments vs delay rate\n",
    "axes[0, 1].bar(range(len(amendments_analysis)), amendments_analysis['Delay_Rate_Pct'], \n",
    "               color='#3498db', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xticks(range(len(amendments_analysis)))\n",
    "axes[0, 1].set_xticklabels(amendments_analysis.index)\n",
    "axes[0, 1].set_ylabel('Delay Rate (%)')\n",
    "axes[0, 1].set_title('Delay Rate by Number of Document Amendments', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Congestion index vs delay rate\n",
    "axes[0, 2].bar(range(len(congestion_analysis)), congestion_analysis['Delay_Rate_Pct'], \n",
    "               color='#f39c12', alpha=0.7, edgecolor='black')\n",
    "axes[0, 2].set_xticks(range(len(congestion_analysis)))\n",
    "axes[0, 2].set_xticklabels(congestion_analysis.index, rotation=15, ha='right')\n",
    "axes[0, 2].set_ylabel('Delay Rate (%)')\n",
    "axes[0, 2].set_title('Delay Rate by Congestion Index', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Weight vs delay rate\n",
    "axes[1, 0].bar(range(len(weight_analysis)), weight_analysis['Delay_Rate_Pct'], \n",
    "               color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xticks(range(len(weight_analysis)))\n",
    "axes[1, 0].set_xticklabels(weight_analysis.index, rotation=15, ha='right')\n",
    "axes[1, 0].set_ylabel('Delay Rate (%)')\n",
    "axes[1, 0].set_title('Delay Rate by Weight Quartile', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Value vs delay rate\n",
    "axes[1, 1].bar(range(len(value_analysis)), value_analysis['Delay_Rate_Pct'], \n",
    "               color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xticks(range(len(value_analysis)))\n",
    "axes[1, 1].set_xticklabels(value_analysis.index, rotation=15, ha='right')\n",
    "axes[1, 1].set_ylabel('Delay Rate (%)')\n",
    "axes[1, 1].set_title('Delay Rate by Value Quartile', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Scatter: doc_completeness_score vs delay_flag\n",
    "axes[1, 2].scatter(df['doc_completeness_score'], df['delayed_flag'], \n",
    "                   alpha=0.1, s=10, color='#e74c3c')\n",
    "# Add trend line\n",
    "z = np.polyfit(df['doc_completeness_score'], df['delayed_flag'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1, 2].plot(df['doc_completeness_score'].sort_values(), \n",
    "                p(df['doc_completeness_score'].sort_values()), \n",
    "                \"r--\", alpha=0.8, linewidth=2, label='Trend')\n",
    "axes[1, 2].set_xlabel('Document Completeness Score')\n",
    "axes[1, 2].set_ylabel('Delayed Flag (0/1)')\n",
    "axes[1, 2].set_title('Document Completeness vs Delay (with Trend)', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\nCORRELATION ANALYSIS - Early Red Flags:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"doc_completeness_score vs delayed_flag: {df['doc_completeness_score'].corr(df['delayed_flag']):.4f}\")\n",
    "print(f\"doc_amendments vs delayed_flag: {df['doc_amendments'].corr(df['delayed_flag']):.4f}\")\n",
    "print(f\"congestion_index vs delayed_flag: {df['congestion_index'].corr(df['delayed_flag']):.4f}\")\n",
    "print(f\"gross_weight_kg vs total_processing_hours: {df['gross_weight_kg'].corr(df['total_processing_hours']):.4f}\")\n",
    "print(f\"declared_value_usd vs total_processing_hours: {df['declared_value_usd'].corr(df['total_processing_hours']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights summary\n",
    "print(\"=\" * 100)\n",
    "print(\"DESCRIPTIVE ANALYTICS - KEY INSIGHTS SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n1. OVERALL DELAY LANDSCAPE:\")\n",
    "print(f\"   • Overall delay rate: {delay_rate:.2f}%\")\n",
    "print(f\"   • Mean processing time: {df['total_processing_hours'].mean():.2f} hours\")\n",
    "print(f\"   • Median processing time: {df['total_processing_hours'].median():.2f} hours\")\n",
    "print(f\"   • P90 processing time: {df['total_processing_hours'].quantile(0.90):.2f} hours\")\n",
    "if len(delayed_df) > 0:\n",
    "    print(f\"   • Mean delay hours (for delayed): {delayed_df['delay_hours'].mean():.2f} hours\")\n",
    "\n",
    "print(\"\\n2. ORIGIN COUNTRY INSIGHTS:\")\n",
    "top_origin = origin_analysis.head(3)\n",
    "print(f\"   • Top 3 origin countries by delay rate:\")\n",
    "for idx, (country, row) in enumerate(top_origin.iterrows(), 1):\n",
    "    print(f\"     {idx}. {country}: {row['Delay_Rate_Pct']:.2f}% delay rate, \"\n",
    "          f\"{row['Avg_Processing_Hours']:.2f}h avg processing\")\n",
    "\n",
    "print(\"\\n3. DESTINATION COUNTRY INSIGHTS:\")\n",
    "top_dest = dest_analysis.head(3)\n",
    "print(f\"   • Top 3 destination countries by delay rate:\")\n",
    "for idx, (country, row) in enumerate(top_dest.iterrows(), 1):\n",
    "    print(f\"     {idx}. {country}: {row['Delay_Rate_Pct']:.2f}% delay rate, \"\n",
    "          f\"{row['Avg_Processing_Hours']:.2f}h avg processing\")\n",
    "\n",
    "print(\"\\n4. TIME-BASED INSIGHTS:\")\n",
    "print(f\"   • Night consignments delay rate: {night_df['delayed_flag'].mean()*100:.2f}%\")\n",
    "print(f\"   • Day consignments delay rate: {day_df['delayed_flag'].mean()*100:.2f}%\")\n",
    "print(f\"   • Weekend created delay rate: {weekend_delay:.2f}%\")\n",
    "print(f\"   • Weekday created delay rate: {weekday_delay:.2f}%\")\n",
    "\n",
    "print(\"\\n5. COMMODITY INSIGHTS:\")\n",
    "top_commodity = commodity_analysis.head(3)\n",
    "print(f\"   • Top 3 commodities by delay rate:\")\n",
    "for idx, (commodity, row) in enumerate(top_commodity.iterrows(), 1):\n",
    "    print(f\"     {idx}. {commodity}: {row['Delay_Rate_Pct']:.2f}% delay rate\")\n",
    "\n",
    "print(\"\\n6. EARLY RED-FLAG INSIGHTS:\")\n",
    "print(f\"   • Document completeness correlation: {df['doc_completeness_score'].corr(df['delayed_flag']):.4f}\")\n",
    "print(f\"   • Document amendments correlation: {df['doc_amendments'].corr(df['delayed_flag']):.4f}\")\n",
    "print(f\"   • Congestion index correlation: {df['congestion_index'].corr(df['delayed_flag']):.4f}\")\n",
    "\n",
    "print(\"\\n7. PORT PERFORMANCE INSIGHTS:\")\n",
    "print(f\"   • Origin ports analyzed: {len(origin_port_analysis)}\")\n",
    "print(f\"   • Destination ports analyzed: {len(dest_port_analysis)}\")\n",
    "top_origin_port = origin_port_analysis.head(1)\n",
    "if len(top_origin_port) > 0:\n",
    "    port_name = top_origin_port.index[0]\n",
    "    print(f\"   • Highest delay rate origin port: {port_name} \"\n",
    "          f\"({top_origin_port.loc[port_name, 'Delay_Rate_Pct']:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"END OF DESCRIPTIVE ANALYTICS\")\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# DEFINE FEATURES AND TARGET\n",
    "# -----------------------------\n",
    "# These are EARLY-WARNING features available before clearance is completed\n",
    "\n",
    "baseline_features = [\n",
    "    'shipment_mode',           # Air / Sea / Road etc.\n",
    "    'commodity',               # Type of horticultural product\n",
    "    'hs_code',                 # Harmonized System code\n",
    "    'origin_country',          # Exporting country\n",
    "    'destination_country',     # Importing country\n",
    "    'exporter_profile',        # Exporter risk/profile category\n",
    "    'doc_completeness_score',  # Quality/completeness of documentation\n",
    "    'missing_docs_proxy',      # Indicator/count of missing documents\n",
    "    'doc_amendments',          # Number of document corrections\n",
    "    'congestion_index',        # System-level congestion indicator\n",
    "    'is_weekend_created',      # Whether shipment was created on weekend\n",
    "    'gross_weight_kg',         # Shipment weight\n",
    "    'declared_value_usd'       # Declared customs value\n",
    "]\n",
    "\n",
    "# Assign X = predictors, y = target\n",
    "X = df[baseline_features]\n",
    "y = df['delayed_flag']  # 1 = delayed, 0 = not delayed\n",
    "\n",
    "# -----------------------------\n",
    "# GROUP FEATURES BY TYPE\n",
    "# -----------------------------\n",
    "# This allows correct preprocessing for each feature category\n",
    "\n",
    "categorical_features = [\n",
    "    'shipment_mode', 'commodity', 'hs_code',\n",
    "    'origin_country', 'destination_country', 'exporter_profile'\n",
    "]\n",
    "\n",
    "numeric_features = [\n",
    "    'doc_completeness_score', 'missing_docs_proxy', 'doc_amendments',\n",
    "    'congestion_index', 'gross_weight_kg', 'declared_value_usd'\n",
    "]\n",
    "\n",
    "binary_features = ['is_weekend_created']\n",
    "\n",
    "# -----------------------------\n",
    "# TRAIN–TEST SPLIT\n",
    "# -----------------------------\n",
    "# Stratification ensures the delay/non-delay ratio is preserved\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,        # 80% train, 20% test\n",
    "    stratify=y,           # Preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# PREPROCESSING PIPELINE\n",
    "# -----------------------------\n",
    "# - Categorical features: One-hot encoding\n",
    "# - Numeric features: Standard scaling\n",
    "# - Binary features: Passed through unchanged\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            'cat',\n",
    "            OneHotEncoder(\n",
    "                handle_unknown='ignore',  # Handle unseen categories safely\n",
    "                min_frequency=0.01         # Reduce high-cardinality noise (e.g. HS codes)\n",
    "            ),\n",
    "            categorical_features\n",
    "        ),\n",
    "        (\n",
    "            'num',\n",
    "            StandardScaler(),             # Normalize numeric ranges\n",
    "            numeric_features\n",
    "        ),\n",
    "        (\n",
    "            'bin',\n",
    "            'passthrough',                # Binary feature needs no transformation\n",
    "            binary_features\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# BASELINE MODEL: LOGISTIC REGRESSION\n",
    "# ================================\n",
    "# Purpose:\n",
    "# - Establish a transparent, interpretable baseline\n",
    "# - Understand direction and strength of predictors\n",
    "\n",
    "log_reg_pipeline = Pipeline(steps=[\n",
    "('preprocessor', preprocessor),\n",
    "(\n",
    "'classifier',\n",
    "LogisticRegression(\n",
    "max_iter=1000, # Ensure convergence\n",
    "class_weight='balanced', # Handle class imbalance\n",
    "n_jobs=-1\n",
    ")\n",
    ")\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Generate predictions and probabilities\n",
    "y_pred_lr = log_reg_pipeline.predict(X_test)\n",
    "y_prob_lr = log_reg_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Logistic Regression – Baseline Features\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# XGBOOST MODEL: BASELINE FEATURES\n",
    "# ================================\n",
    "# Purpose:\n",
    "# - Gradient boosting model for improved performance\n",
    "# - Uses same baseline features as logistic regression\n",
    "\n",
    "# Transform the data using the preprocessor\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Generate predictions and probabilities\n",
    "y_pred_xgb = xgb_model.predict(X_test_transformed)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"XGBoost – Baseline Features\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_xgb))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# ================================\n",
    "# LIGHTGBM MODEL: BASELINE FEATURES\n",
    "# ================================\n",
    "# Purpose:\n",
    "# - Light gradient boosting model for fast training\n",
    "# - Uses same baseline features as logistic regression\n",
    "\n",
    "# LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgb_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Generate predictions and probabilities\n",
    "y_pred_lgb = lgb_model.predict(X_test_transformed)\n",
    "y_prob_lgb = lgb_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"LightGBM – Baseline Features\")\n",
    "print(classification_report(y_test, y_pred_lgb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the features and the target\n",
    "\n",
    "X = df[baseline_features]\n",
    "y = df['delayed_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', min_frequency=0.01), categorical_features),\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('bin', 'passthrough', binary_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest pipeline\n",
    "clf = Pipeline([('preprocessor', preprocessor),\n",
    "                   ('model', RandomForestClassifier(n_estimators = 200,\n",
    "                                                  max_depth = None,\n",
    "                                                  random_state=42,\n",
    "                                                  class_weight = 'balanced'))])\n",
    "\n",
    "#This ensures prerocessing is applied consistently to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training of the Random forest model\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of the Random Forest model\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy 0.799(~0.8) means the model correctly predicted whether a shipment was delayed in 8 out of ten cases on the test set.\n",
    "\n",
    "The model perfomed well for both the delayed and non-delayed shipments.\n",
    "\n",
    "Non-delayed shipment had a higher recall meaning the model captured this shipments better than the delayed ones.\n",
    "\n",
    "The model is not biased as bothe precision and recall is balanced on both classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------- Confusion Matrix ---------\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm,annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------- ROC Curve -------\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc (fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve')\n",
    "plt.plot([0,1], [0,1], color='navy', lw=2, linestyle = '--')\n",
    "plt.title('ROC Curve-Random Forest')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------Precision Recall Curve -----\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, color = 'maroon', lw=2)\n",
    "plt.title('Precision Recall Curve - Random Forest')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ------Feature Importance -----\n",
    "\n",
    "#extract rf from the pipeline\n",
    "rf_model = clf.named_steps['model']\n",
    "\n",
    "#Feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "#feature names after preprocessing\n",
    "feature_names = clf.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "#create DataFrame\n",
    "feat_importances = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#plot top 10 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_importances.head(10), palette='viridis')\n",
    "plt.title('Top 10 Feature Importance - Random Forest')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The confusion matrix gives a view on misclassifications\n",
    "\n",
    "The ROC curve shows how the model can discriminate between the two classes\n",
    "\n",
    "Precision curve shows trade-off between catching delays vs avoiding false alarms\n",
    "\n",
    "Random forest gives feature importance scores that help in understanding which logistic factors drive delays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
